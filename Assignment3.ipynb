{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2BzUTj5qAzL",
        "colab_type": "text"
      },
      "source": [
        "##Question 1: Network Compression using SVD\n",
        "\n",
        "Importing library files, disabling warnings and reading the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-yOcPlgrSOO",
        "colab_type": "code",
        "outputId": "6ccc853b-e7bb-4a78-e7fd-a22d313eb042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import librosa\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahtO-uh3r9gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_nodes_h1 = 1024\n",
        "n_nodes_h2 = 1024\n",
        "n_nodes_h3 = 1024\n",
        "n_nodes_h4 = 1024\n",
        "n_nodes_h5 = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiWkSOmwsGNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 10\n",
        "batch_size = 300\n",
        "\n",
        "#height x weight\n",
        "x = tf.placeholder('float',[None, 784])\n",
        "y = tf.placeholder('float')\n",
        "initializer = tf.random_normal_initializer()\n",
        "bias_init = tf.zeros_initializer()\n",
        "\n",
        "input_dim = 784\n",
        "\n",
        "def neural_network_model(data):\n",
        "\n",
        "    hidden_1_layer = {'weights': tf.Variable(initializer([input_dim,n_nodes_h1])),\n",
        "                     'biases': tf.Variable(bias_init([n_nodes_h1]))}\n",
        "    \n",
        "    hidden_2_layer = {'weights': tf.Variable(initializer([n_nodes_h1,n_nodes_h2])),\n",
        "                     'biases': tf.Variable(bias_init([n_nodes_h2]))}\n",
        "    \n",
        "    hidden_3_layer = {'weights': tf.Variable(initializer([n_nodes_h2,n_nodes_h3])),\n",
        "                     'biases': tf.Variable(bias_init([n_nodes_h3]))}\n",
        "    \n",
        "    hidden_4_layer = {'weights': tf.Variable(initializer([n_nodes_h3,n_nodes_h4])),\n",
        "                     'biases': tf.Variable(bias_init([n_nodes_h4]))}\n",
        "    \n",
        "    hidden_5_layer = {'weights': tf.Variable(initializer([n_nodes_h4,n_nodes_h5])),\n",
        "                     'biases': tf.Variable(bias_init([n_nodes_h5]))}\n",
        "    \n",
        "    output_layer = {'weights': tf.Variable(initializer([n_nodes_h5,n_classes])),\n",
        "                     'biases': tf.Variable(bias_init([n_classes]))}\n",
        "    \n",
        "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'])\n",
        "    l1 = tf.nn.relu(l1)\n",
        "    \n",
        "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'])\n",
        "    l2 = tf.nn.relu(l2)\n",
        "    \n",
        "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'])\n",
        "    l3 = tf.nn.relu(l3)\n",
        "    \n",
        "    l4 = tf.add(tf.matmul(l3,hidden_4_layer['weights']),hidden_4_layer['biases'])\n",
        "    l4 = tf.nn.relu(l4)\n",
        "    \n",
        "    l5 = tf.add(tf.matmul(l4,hidden_5_layer['weights']),hidden_5_layer['biases'])\n",
        "    l5 = tf.nn.relu(l5)\n",
        "    \n",
        "    output = tf.matmul(l5,output_layer['weights']) + output_layer['biases']\n",
        "    \n",
        "    return l1,l2,l3,l4,l5,output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "29d84e3c-8806-48bf-f4be-108a8fb4e820",
        "id": "DtvSoHGl4ley",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "prediction_l1,prediction_l2,prediction_l3,prediction_l4,prediction_l5,prediction = neural_network_model(x)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
        "\n",
        "hm_epochs = 20\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for epoch in range(hm_epochs):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "      epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "      _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
        "      epoch_loss += c\n",
        "\n",
        "    correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
        "\n",
        "    print('Epoch', epoch+1, 'completed out of', hm_epochs, 'loss:', epoch_loss, 'Accuracy:', accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 20 loss: 3189269175.5 Accuracy: 0.6853\n",
            "Epoch 2 completed out of 20 loss: 833312243.25 Accuracy: 0.8008\n",
            "Epoch 3 completed out of 20 loss: 545752036.375 Accuracy: 0.8436\n",
            "Epoch 4 completed out of 20 loss: 412647805.125 Accuracy: 0.8657\n",
            "Epoch 5 completed out of 20 loss: 329995977.75 Accuracy: 0.8768\n",
            "Epoch 6 completed out of 20 loss: 272104342.375 Accuracy: 0.886\n",
            "Epoch 7 completed out of 20 loss: 226613852.125 Accuracy: 0.893\n",
            "Epoch 8 completed out of 20 loss: 192514779.90625 Accuracy: 0.8977\n",
            "Epoch 9 completed out of 20 loss: 163263046.625 Accuracy: 0.9018\n",
            "Epoch 10 completed out of 20 loss: 137969729.75 Accuracy: 0.9059\n",
            "Epoch 11 completed out of 20 loss: 118127623.859375 Accuracy: 0.907\n",
            "Epoch 12 completed out of 20 loss: 100531864.2578125 Accuracy: 0.91\n",
            "Epoch 13 completed out of 20 loss: 86600886.25 Accuracy: 0.9125\n",
            "Epoch 14 completed out of 20 loss: 71087507.0703125 Accuracy: 0.9143\n",
            "Epoch 15 completed out of 20 loss: 61722327.37890625 Accuracy: 0.9137\n",
            "Epoch 16 completed out of 20 loss: 49597379.15625 Accuracy: 0.9146\n",
            "Epoch 17 completed out of 20 loss: 43171156.54003906 Accuracy: 0.9156\n",
            "Epoch 18 completed out of 20 loss: 35979102.947265625 Accuracy: 0.9176\n",
            "Epoch 19 completed out of 20 loss: 28268536.49609375 Accuracy: 0.9181\n",
            "Epoch 20 completed out of 20 loss: 23309778.04145813 Accuracy: 0.9192\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Exn3Ah7gWXb",
        "colab_type": "code",
        "outputId": "55981ae6-186a-4098-ac3a-143560e0fb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "variables"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'Variable:0' shape=(784, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_1:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_2:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_3:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_4:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_5:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_6:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_7:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_8:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_9:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_10:0' shape=(1024, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_11:0' shape=(10,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_12:0' shape=(784, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_13:0' shape=(20, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_14:0' shape=(1024, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_15:0' shape=(20, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_16:0' shape=(1024, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_17:0' shape=(20, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_18:0' shape=(1024, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_19:0' shape=(20, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_20:0' shape=(1024, 20) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_21:0' shape=(20, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_22:0' shape=(784, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_23:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_24:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_25:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_26:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_27:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_28:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_29:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_30:0' shape=(1024, 1024) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_31:0' shape=(1024,) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_32:0' shape=(1024, 10) dtype=float32_ref>,\n",
              " <tf.Variable 'Variable_33:0' shape=(10,) dtype=float32_ref>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmvdZ9oCeVxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight1 = variables[0]\n",
        "bias1 = variables[1]\n",
        "\n",
        "weight2 = variables[2]\n",
        "bias2 = variables[3]\n",
        "\n",
        "weight3 = variables[4]\n",
        "bias3 = variables[5]\n",
        "\n",
        "weight4 = variables[6]\n",
        "bias4 = variables[7]\n",
        "\n",
        "weight5 = variables[8]\n",
        "bias5 = variables[9]\n",
        "\n",
        "weight6 = variables[10]\n",
        "bias6 = variables[11]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TA0Gqud96N7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer1_s,layer1_u,layer1_v = tf.svd(weight1)\n",
        "layer2_s,layer2_u,layer2_v = tf.svd(weight2)\n",
        "layer3_s,layer3_u,layer3_v = tf.svd(weight3)\n",
        "layer4_s,layer4_u,layer4_v = tf.svd(weight4)\n",
        "layer5_s,layer5_u,layer5_v = tf.svd(weight5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRNoqdm4wCNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SVD Compression Loop Begins here\n",
        "D = [10,20,50,100,200,'full']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QymGJHHjJwM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def compressed_neural_network_model(data,weight_p1,weight_p2,weight_p3,weight_p4,weight_p5,weight_p6,\n",
        "                          bias1,bias2,bias3,bias4,bias5,bias6):\n",
        "    \n",
        "      hidden_1_layer = {'weights': weight_p1,\n",
        "                      'biases': bias1}\n",
        "      \n",
        "      hidden_2_layer = {'weights': weight_p2,\n",
        "                      'biases': bias2}\n",
        "      \n",
        "      hidden_3_layer = {'weights': weight_p3,\n",
        "                      'biases': bias3}\n",
        "      \n",
        "      hidden_4_layer = {'weights': weight_p4,\n",
        "                      'biases': bias4}\n",
        "      \n",
        "      hidden_5_layer = {'weights': weight_p5,\n",
        "                      'biases': bias5}\n",
        "      \n",
        "      output_layer = {'weights': weight_p6,\n",
        "                      'biases': bias6}\n",
        "\n",
        "      l1 = tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'])\n",
        "      l1 = tf.nn.relu(l1)\n",
        "      \n",
        "      l2 = tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'])\n",
        "      l2 = tf.nn.relu(l2)\n",
        "      \n",
        "      l3 = tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'])\n",
        "      l3 = tf.nn.relu(l3)\n",
        "      \n",
        "      l4 = tf.add(tf.matmul(l3,hidden_4_layer['weights']),hidden_4_layer['biases'])\n",
        "      l4 = tf.nn.relu(l4)\n",
        "      \n",
        "      l5 = tf.add(tf.matmul(l4,hidden_5_layer['weights']),hidden_5_layer['biases'])\n",
        "      l5 = tf.nn.relu(l5)\n",
        "      \n",
        "      output = tf.matmul(l5,output_layer['weights']) + output_layer['biases']\n",
        "\n",
        "      return l1,l2,l3,l4,l5,output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXBtkpkgfnLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def svd(d_value):\n",
        "\n",
        "  if d_value == 'full':\n",
        "\n",
        "    weight_p1 = tf.Variable(tf.matmul(tf.matmul(layer1_u, tf.diag(layer1_s)), layer1_v, transpose_b=True))\n",
        "    weight_p2 = tf.Variable(tf.matmul(tf.matmul(layer2_u, tf.diag(layer2_s)), layer2_v, transpose_b=True))\n",
        "    weight_p3 = tf.Variable(tf.matmul(tf.matmul(layer3_u, tf.diag(layer3_s)), layer3_v, transpose_b=True))\n",
        "    weight_p4 = tf.Variable(tf.matmul(tf.matmul(layer4_u, tf.diag(layer4_s)), layer4_v, transpose_b=True))\n",
        "    weight_p5 = tf.Variable(tf.matmul(tf.matmul(layer5_u, tf.diag(layer5_s)), layer5_v, transpose_b=True))\n",
        "    weight_p6 = weight6\n",
        "  \n",
        "  else:\n",
        "\n",
        "    weight_p1 = tf.Variable(tf.matmul(tf.matmul(layer1_u[:,0:d_value], tf.diag(layer1_s[0:d_value])), layer1_v[:,0:d_value], transpose_b=True))\n",
        "    weight_p2 = tf.Variable(tf.matmul(tf.matmul(layer2_u[:,0:d_value], tf.diag(layer2_s[0:d_value])), layer2_v[:,0:d_value], transpose_b=True))\n",
        "    weight_p3 = tf.Variable(tf.matmul(tf.matmul(layer3_u[:,0:d_value], tf.diag(layer3_s[0:d_value])), layer3_v[:,0:d_value], transpose_b=True))\n",
        "    weight_p4 = tf.Variable(tf.matmul(tf.matmul(layer4_u[:,0:d_value], tf.diag(layer4_s[0:d_value])), layer4_v[:,0:d_value], transpose_b=True))\n",
        "    weight_p5 = tf.Variable(tf.matmul(tf.matmul(layer5_u[:,0:d_value], tf.diag(layer5_s[0:d_value])), layer5_v[:,0:d_value], transpose_b=True))\n",
        "    weight_p6 = weight6\n",
        "\n",
        "  #height x weight\n",
        "  x = tf.placeholder('float',[None, 784])\n",
        "  y = tf.placeholder('float')\n",
        "  initializer = tf.random_normal_initializer()\n",
        "  bias_init = tf.zeros_initializer()\n",
        "\n",
        "  input_dim = 784\n",
        "\n",
        "  prediction_l1,prediction_l2,prediction_l3,prediction_l4,prediction_l5,prediction = compressed_neural_network_model(x,weight_p1,weight_p2,weight_p3,weight_p4,weight_p5,weight_p6,\n",
        "                          bias1,bias2,bias3,bias4,bias5,bias6)\n",
        "  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y))\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "  hm_epochs = 20\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(hm_epochs):\n",
        "      epoch_loss = 0\n",
        "\n",
        "      \n",
        "      for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "        epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
        "\n",
        "        epoch_loss += c\n",
        "\n",
        "      correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
        "      accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
        "\n",
        "    test_images = mnist.test.images\n",
        "    last_layer = sess.run(prediction,feed_dict = {x:test_images})\n",
        "    count = 0\n",
        "    for i in range(0,10000):\n",
        "      \n",
        "      if np.argmax(last_layer[i]) == np.argmax(mnist.test.labels[i]):\n",
        "        count += 1\n",
        "    \n",
        "  return (count/10000)\n",
        "  \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wa4sFyrieKl",
        "colab_type": "code",
        "outputId": "d2b6e822-5d03-4fb2-ff84-93d2bcc50653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "accuracy_list = []\n",
        "for i in D:\n",
        "  print(\"D Value is\",i)\n",
        "  accuracy_list.append(svd(i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D Value is 10\n",
            "D Value is 20\n",
            "D Value is 50\n",
            "D Value is 100\n",
            "D Value is 200\n",
            "D Value is full\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLlQepuWKD3-",
        "colab_type": "code",
        "outputId": "8ee870a2-1e64-4d2f-f787-f0d1b13e4a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot(D, accuracy_list)\n",
        "plt.xlabel('D Values')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSgEhIiJJKSBERCQpBYSIiCT1/wEzKbZWn6Q0qQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC_RvmgZX3kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Question 1.6\n",
        "def new_neural_network_model(data,u_p1,u_p2,u_p3,u_p4,u_p5,v_p1,v_p2,v_p3,v_p4,v_p5,weight6,\n",
        "                             bias1,bias2,bias3,bias4,bias5,bias6):\n",
        "    \n",
        "      hidden_1_layer = {'u_val': u_p1,\n",
        "                        'v_val': v_p1,\n",
        "                        'biases': bias1}\n",
        "      \n",
        "      hidden_2_layer = {'u_val': u_p2,\n",
        "                        'v_val': v_p2,\n",
        "                        'biases': bias2}\n",
        "      \n",
        "      hidden_3_layer = {'u_val': u_p3,\n",
        "                        'v_val': v_p3,\n",
        "                        'biases': bias3}\n",
        "      \n",
        "      hidden_4_layer = {'u_val': u_p4,\n",
        "                        'v_val': v_p4,\n",
        "                        'biases': bias4}\n",
        "      \n",
        "      hidden_5_layer = {'u_val': u_p5,\n",
        "                        'v_val': v_p5,\n",
        "                        'biases': bias5}\n",
        "      \n",
        "      output_layer = {'weights': weight6,\n",
        "                      'biases': bias6}\n",
        "\n",
        "      l1 = tf.add(tf.matmul(data,tf.matmul(hidden_1_layer['u_val'],hidden_1_layer['v_val'])),\n",
        "                            hidden_1_layer['biases'])\n",
        "      l1 = tf.nn.relu(l1)\n",
        "      \n",
        "      l2 = tf.add(tf.matmul(l1,tf.matmul(hidden_2_layer['u_val'],hidden_2_layer['v_val'])),\n",
        "                            hidden_2_layer['biases'])\n",
        "      l2 = tf.nn.relu(l2)\n",
        "      \n",
        "      l3 = tf.add(tf.matmul(l2,tf.matmul(hidden_3_layer['u_val'],hidden_3_layer['v_val'])),\n",
        "                            hidden_3_layer['biases'])\n",
        "      l3 = tf.nn.relu(l3)\n",
        "      \n",
        "      l4 = tf.add(tf.matmul(l3,tf.matmul(hidden_4_layer['u_val'],hidden_4_layer['v_val'])),\n",
        "                            hidden_4_layer['biases'])\n",
        "      l4 = tf.nn.relu(l4)\n",
        "      \n",
        "      l5 = tf.add(tf.matmul(l4,tf.matmul(hidden_5_layer['u_val'],hidden_5_layer['v_val'])),\n",
        "                            hidden_5_layer['biases'])\n",
        "      l5 = tf.nn.relu(l5)\n",
        "      \n",
        "      output = tf.matmul(l5,output_layer['weights']) + output_layer['biases']\n",
        "\n",
        "      return l1,l2,l3,l4,l5,output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNXtKqNXuPj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  u_p1,v_p1 = tf.Variable(layer1_u[:,0:20]), tf.Variable(tf.matmul(tf.diag(layer1_s[0:20]), layer1_v[:,0:20],transpose_b=True))\n",
        "  u_p2,v_p2 = tf.Variable(layer2_u[:,0:20]), tf.Variable(tf.matmul(tf.diag(layer2_s[0:20]), layer2_v[:,0:20],transpose_b=True))\n",
        "  u_p3,v_p3 = tf.Variable(layer3_u[:,0:20]), tf.Variable(tf.matmul(tf.diag(layer3_s[0:20]), layer3_v[:,0:20],transpose_b=True))\n",
        "  u_p4,v_p4 = tf.Variable(layer4_u[:,0:20]), tf.Variable(tf.matmul(tf.diag(layer4_s[0:20]), layer4_v[:,0:20],transpose_b=True))\n",
        "  u_p5,v_p5 = tf.Variable(layer5_u[:,0:20]), tf.Variable(tf.matmul(tf.diag(layer5_s[0:20]), layer5_v[:,0:20],transpose_b=True))\n",
        "  weight_p6 = weight6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcps90jVj8io",
        "colab_type": "code",
        "outputId": "9acb9323-1ac8-49a1-aa18-e4c860d0f99c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "#height x weight\n",
        "x = tf.placeholder('float',[None, 784])\n",
        "y = tf.placeholder('float',[None, 10])\n",
        "initializer = tf.random_normal_initializer()\n",
        "bias_init = tf.zeros_initializer()\n",
        "\n",
        "input_dim = 784\n",
        "\n",
        "prediction_l1,prediction_l2,prediction_l3,prediction_l4,prediction_l5,prediction = new_neural_network_model(x,u_p1,u_p2,u_p3,u_p4,u_p5,v_p1,v_p2,v_p3,v_p4,v_p5,weight_p6,\n",
        "                        bias1,bias2,bias3,bias4,bias5,bias6)\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
        "correct = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct,'float'))\n",
        "\n",
        "hm_epochs = 20\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for epoch in range(hm_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for _ in range(int(mnist.train.num_examples/batch_size)):\n",
        "      epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
        "\n",
        "      _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
        "      accTest = accuracy.eval({x:mnist.test.images, y:mnist.test.labels})\n",
        "      \n",
        "      epoch_loss += c\n",
        "\n",
        "    print('Epoch', epoch+1, 'completed out of', hm_epochs, 'Accuracy:', accTest)\n",
        "      \n",
        "  count = 0\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 20 Accuracy: 0.5776\n",
            "Epoch 2 completed out of 20 Accuracy: 0.6676\n",
            "Epoch 3 completed out of 20 Accuracy: 0.7139\n",
            "Epoch 4 completed out of 20 Accuracy: 0.7366\n",
            "Epoch 5 completed out of 20 Accuracy: 0.7551\n",
            "Epoch 6 completed out of 20 Accuracy: 0.7626\n",
            "Epoch 7 completed out of 20 Accuracy: 0.7823\n",
            "Epoch 8 completed out of 20 Accuracy: 0.7531\n",
            "Epoch 9 completed out of 20 Accuracy: 0.777\n",
            "Epoch 10 completed out of 20 Accuracy: 0.8067\n",
            "Epoch 11 completed out of 20 Accuracy: 0.7942\n",
            "Epoch 12 completed out of 20 Accuracy: 0.8048\n",
            "Epoch 13 completed out of 20 Accuracy: 0.8231\n",
            "Epoch 14 completed out of 20 Accuracy: 0.8399\n",
            "Epoch 15 completed out of 20 Accuracy: 0.8197\n",
            "Epoch 16 completed out of 20 Accuracy: 0.8204\n",
            "Epoch 17 completed out of 20 Accuracy: 0.818\n",
            "Epoch 18 completed out of 20 Accuracy: 0.8448\n",
            "Epoch 19 completed out of 20 Accuracy: 0.851\n",
            "Epoch 20 completed out of 20 Accuracy: 0.832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENSedBK6LWLO",
        "colab_type": "text"
      },
      "source": [
        "##Question 2: RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HebGfvGpLUJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mount the drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m370Cr5nqqZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extractPickle(pickle_file , mag_pickle_file, file_path):\n",
        "  if os.path.exists(pickle_file) and os.path.exists(mag_pickle_file):\n",
        "    speech_list = pickle.load(open(pickle_file, 'rb'))\n",
        "    mag_speech_list = pickle.load(open(mag_pickle_file, 'rb'))\n",
        "    \n",
        "    return speech_list , mag_speech_list\n",
        "  \n",
        "  else:\n",
        "    speech_list = []\n",
        "    mag_speech_list = []\n",
        "    for file in sorted(glob.iglob(file_path)):\n",
        "      s,sr = librosa.load(file , sr=None)\n",
        "      S = librosa.stft(s, n_fft=1024, hop_length=512)\n",
        "      speech_list.append(S)\n",
        "\n",
        "      mag_S = np.abs(S)\n",
        "      mag_speech_list.append(mag_S)\n",
        "\n",
        "    pickle.dump(speech_list, open(pickle_file, 'wb'))\n",
        "    pickle.dump(mag_speech_list, open(mag_pickle_file, 'wb'))\n",
        "    \n",
        "    return speech_list , mag_speech_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sD9qbpiOZFY",
        "colab_type": "code",
        "outputId": "9545aa8b-b08f-41f0-ae2b-f564680a3579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ha5l7Ku-6uI2",
        "colab_type": "code",
        "outputId": "570eef85-74bb-4d56-d268-16b681242faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "folder_path = '/content/drive/My Drive/lstm/'\n",
        "\n",
        "trs_path = folder_path + 'tr/trs*.wav'\n",
        "trn_path = folder_path + 'tr/trn*.wav'\n",
        "trx_path = folder_path + 'tr/trx*.wav'\n",
        "\n",
        "vs_path = folder_path + 'v/vs*.wav'\n",
        "vn_path = folder_path + 'v/vn*.wav'\n",
        "vx_path = folder_path + 'v/vx*.wav'\n",
        "\n",
        "tex_path = folder_path + 'te/tex*.wav'\n",
        "\n",
        "trs_pickle = folder_path + 'trs_pickle.pkl'\n",
        "mag_trs_pickle = folder_path + 'mag_trs_pickle.pkl'\n",
        "trn_pickle = folder_path + 'trn_pickle.pkl'\n",
        "mag_trn_pickle = folder_path + 'mag_trn_pickle.pkl'\n",
        "trx_pickle = folder_path + 'trx_pickle.pkl'\n",
        "mag_trx_pickle = folder_path + 'mag_trx_pickle.pkl'\n",
        "\n",
        "vs_pickle = folder_path + 'vs_pickle.pkl'\n",
        "mag_vs_pickle = folder_path + 'mag_vs_pickle.pkl'\n",
        "vn_pickle = folder_path + 'vn_pickle.pkl'\n",
        "mag_vn_pickle = folder_path + 'mag_vn_pickle.pkl'\n",
        "vx_pickle = folder_path + 'vx_pickle.pkl'\n",
        "mag_vx_pickle = folder_path + 'mag_vx_pickle.pkl'\n",
        "\n",
        "tex_pickle = folder_path + 'tex_pickle.pkl'\n",
        "mag_tex_pickle = folder_path + 'mag_tex_pickle.pkl'\n",
        "\n",
        "trs_list, mag_trs_list = extractPickle(trs_pickle, mag_trs_pickle, trs_path)\n",
        "print(\"Train signal loaded\")\n",
        "print(len(trs_list),len(mag_trs_list))\n",
        "\n",
        "trn_list, mag_trn_list = extractPickle(trn_pickle, mag_trn_pickle, trn_path)\n",
        "print(\"Train noise loaded\")\n",
        "print(len(trn_list),len(mag_trn_list))\n",
        "\n",
        "trx_list, mag_trx_list = extractPickle(trx_pickle, mag_trx_pickle, trx_path)\n",
        "print(\"Train mixed loaded\")\n",
        "print(len(trx_list),len(mag_trx_list))\n",
        "\n",
        "vs_list, mag_vs_list = extractPickle(vs_pickle, mag_vs_pickle, vs_path)\n",
        "print(\"Validation signal loaded\")\n",
        "print(len(vs_list),len(mag_vs_list))\n",
        "\n",
        "vn_list, mag_vn_list = extractPickle(vn_pickle, mag_vn_pickle, vn_path)\n",
        "print(\"Validation noise loaded\")\n",
        "print(len(vn_list),len(mag_vn_list))\n",
        "\n",
        "vx_list, mag_vx_list = extractPickle(vx_pickle, mag_vx_pickle, vx_path)\n",
        "print(\"Validation mixed loaded\")\n",
        "print(len(vx_list),len(mag_vx_list))\n",
        "\n",
        "tex_list, mag_tex_list = extractPickle(tex_pickle, mag_tex_pickle, tex_path)\n",
        "print(\"Test mixed loaded\")\n",
        "print(len(tex_list),len(mag_tex_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train signal loaded\n",
            "1200 1200\n",
            "Train noise loaded\n",
            "1200 1200\n",
            "Train mixed loaded\n",
            "1200 1200\n",
            "Validation signal loaded\n",
            "1200 1200\n",
            "Validation noise loaded\n",
            "1200 1200\n",
            "Validation mixed loaded\n",
            "1200 1200\n",
            "Test mixed loaded\n",
            "400 400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cSMP0TZj8-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# All files are loaded\n",
        "def IBM_Matrix(s_list,n_list):\n",
        "  m_list = []\n",
        "  for i in range(0,len(s_list)):\n",
        "    m = 1*(s_list[i] > n_list[i]) \n",
        "    m_list.append(m)\n",
        "  return m_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLIpq9FCLlMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M = IBM_Matrix(mag_trs_list,mag_trn_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz2iM1gLL27X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#So we know that mag_trx_list will be your input and M should be your desired output\n",
        "batch_size = 10\n",
        "hm_epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ago6uEKjfCQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "x = tf.placeholder('float',[None, None, 513])\n",
        "y = tf.placeholder('float',[None, None, 513])\n",
        "\n",
        "def lstm(data):\n",
        "\n",
        "  lstm_cell = tf.contrib.rnn.LSTMCell(513, initializer=tf.contrib.layers.variance_scaling_initializer())\n",
        "  drop_out = tf.nn.rnn_cell.DropoutWrapper(lstm_cell , output_keep_prob=0.9)\n",
        "  lstm_output , state = tf.nn.dynamic_rnn(drop_out, data , dtype=tf.float32)\n",
        "  output = tf.layers.dense(lstm_output , 513 , activation=tf.nn.sigmoid , kernel_initializer=tf.contrib.layers.variance_scaling_initializer())\n",
        "  \n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ZpfaiMqxvJ",
        "colab_type": "code",
        "outputId": "4667ec15-c662-417f-ebbc-cb6768d1db74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "prediction = lstm(x)\n",
        "cost = tf.reduce_mean(tf.square(prediction-y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  for epoch in range(hm_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0,len(mag_trx_list),batch_size):\n",
        "\n",
        "      epoch_x = [np.transpose(mag_trx_list[j]) for j in range(i,i+batch_size)]\n",
        "      epoch_y = [np.transpose(M[j]) for j in range(i,i+batch_size)]\n",
        "\n",
        "      epoch_x = np.array(epoch_x).reshape(batch_size,-1,513)\n",
        "      epoch_y = np.array(epoch_y).reshape(batch_size,-1,513)\n",
        "\n",
        "      _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
        "      epoch_loss += c\n",
        "    \n",
        "    average_loss = epoch_loss/(len(mag_trx_list))\n",
        "\n",
        "    print('Epoch', epoch+1, 'completed out of', hm_epochs, 'loss:', average_loss)\n",
        "  \n",
        "  SNR_Array = []\n",
        "\n",
        "  for i in range(len(mag_vx_list)):\n",
        "\n",
        "    mag_vx_value = np.array([mag_vx_list[i].T])\n",
        "    vx_value = np.array([vx_list[i].T])\n",
        "    \n",
        "    validation_output = sess.run(prediction,feed_dict = {x: mag_vx_value})\n",
        "    s_predict = np.multiply(validation_output,vx_value)\n",
        "    s_predict = s_predict.T[:,:,0]\n",
        "\n",
        "    s_hat = librosa.istft(s_predict , hop_length=512 , win_length=1024)\n",
        "    s = librosa.istft(vs_list[i] , hop_length=512 , win_length=1024)\n",
        "\n",
        "    size = np.shape(s_hat)[0]\n",
        "    num = np.dot(np.transpose(s),s)\n",
        "    den = np.dot(np.transpose(s - s_hat),(s - s_hat))\n",
        "    SNR = 10 * np.log10(num/den)\n",
        "  \n",
        "    SNR_Array.append(SNR)\n",
        "\n",
        "  print(\"\")\n",
        "  print(\"Mean SNR:\", np.mean(SNR_Array))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 100 loss: 0.019253769268592198\n",
            "Epoch 2 completed out of 100 loss: 0.015865754342327516\n",
            "Epoch 3 completed out of 100 loss: 0.015143717477718989\n",
            "Epoch 4 completed out of 100 loss: 0.014577611250181992\n",
            "Epoch 5 completed out of 100 loss: 0.014151911350588004\n",
            "Epoch 6 completed out of 100 loss: 0.013851975568880638\n",
            "Epoch 7 completed out of 100 loss: 0.013540255576372147\n",
            "Epoch 8 completed out of 100 loss: 0.013349853809922934\n",
            "Epoch 9 completed out of 100 loss: 0.013146889004856348\n",
            "Epoch 10 completed out of 100 loss: 0.012925438849876325\n",
            "Epoch 11 completed out of 100 loss: 0.013117407939086358\n",
            "Epoch 12 completed out of 100 loss: 0.012840820314983526\n",
            "Epoch 13 completed out of 100 loss: 0.012470221612602472\n",
            "Epoch 14 completed out of 100 loss: 0.012329267555226881\n",
            "Epoch 15 completed out of 100 loss: 0.012239174582064152\n",
            "Epoch 16 completed out of 100 loss: 0.012041758851458629\n",
            "Epoch 17 completed out of 100 loss: 0.011935671102255583\n",
            "Epoch 18 completed out of 100 loss: 0.011862292593965928\n",
            "Epoch 19 completed out of 100 loss: 0.011741823821018139\n",
            "Epoch 20 completed out of 100 loss: 0.011673518344759942\n",
            "Epoch 21 completed out of 100 loss: 0.011535311893870434\n",
            "Epoch 22 completed out of 100 loss: 0.011501198206096887\n",
            "Epoch 23 completed out of 100 loss: 0.011593416035175323\n",
            "Epoch 24 completed out of 100 loss: 0.011517602590223153\n",
            "Epoch 25 completed out of 100 loss: 0.011328242626041174\n",
            "Epoch 26 completed out of 100 loss: 0.011179286725819111\n",
            "Epoch 27 completed out of 100 loss: 0.01109374850988388\n",
            "Epoch 28 completed out of 100 loss: 0.011059162685026725\n",
            "Epoch 29 completed out of 100 loss: 0.010936866533011198\n",
            "Epoch 30 completed out of 100 loss: 0.010889094863086939\n",
            "Epoch 31 completed out of 100 loss: 0.010877805768201748\n",
            "Epoch 32 completed out of 100 loss: 0.010816894713789224\n",
            "Epoch 33 completed out of 100 loss: 0.010683867887904246\n",
            "Epoch 34 completed out of 100 loss: 0.010644837922106187\n",
            "Epoch 35 completed out of 100 loss: 0.010639253668487073\n",
            "Epoch 36 completed out of 100 loss: 0.010590333503981432\n",
            "Epoch 37 completed out of 100 loss: 0.010539671014994383\n",
            "Epoch 38 completed out of 100 loss: 0.010535803058495124\n",
            "Epoch 39 completed out of 100 loss: 0.010456446018069982\n",
            "Epoch 40 completed out of 100 loss: 0.010388232301920652\n",
            "Epoch 41 completed out of 100 loss: 0.010334523574759563\n",
            "Epoch 42 completed out of 100 loss: 0.010268386385093132\n",
            "Epoch 43 completed out of 100 loss: 0.010218850858509541\n",
            "Epoch 44 completed out of 100 loss: 0.010222832951694728\n",
            "Epoch 45 completed out of 100 loss: 0.010222778245806695\n",
            "Epoch 46 completed out of 100 loss: 0.010238085308422644\n",
            "Epoch 47 completed out of 100 loss: 0.010278151612728834\n",
            "Epoch 48 completed out of 100 loss: 0.010627387035638093\n",
            "Epoch 49 completed out of 100 loss: 0.010916785870989165\n",
            "Epoch 50 completed out of 100 loss: 0.010822649498780569\n",
            "Epoch 51 completed out of 100 loss: 0.010920018125325441\n",
            "Epoch 52 completed out of 100 loss: 0.010617610855648914\n",
            "Epoch 53 completed out of 100 loss: 0.010456529775013526\n",
            "Epoch 54 completed out of 100 loss: 0.010289057940244675\n",
            "Epoch 55 completed out of 100 loss: 0.010112537145614624\n",
            "Epoch 56 completed out of 100 loss: 0.010005275954802832\n",
            "Epoch 57 completed out of 100 loss: 0.009935485627502204\n",
            "Epoch 58 completed out of 100 loss: 0.009893255302061638\n",
            "Epoch 59 completed out of 100 loss: 0.009854388404637576\n",
            "Epoch 60 completed out of 100 loss: 0.009836114073793093\n",
            "Epoch 61 completed out of 100 loss: 0.009799354982872804\n",
            "Epoch 62 completed out of 100 loss: 0.009789420297990242\n",
            "Epoch 63 completed out of 100 loss: 0.009770755140731733\n",
            "Epoch 64 completed out of 100 loss: 0.009764103926718235\n",
            "Epoch 65 completed out of 100 loss: 0.009769423839946587\n",
            "Epoch 66 completed out of 100 loss: 0.009791414812207222\n",
            "Epoch 67 completed out of 100 loss: 0.00980527459954222\n",
            "Epoch 68 completed out of 100 loss: 0.009806538869937261\n",
            "Epoch 69 completed out of 100 loss: 0.009780642626186211\n",
            "Epoch 70 completed out of 100 loss: 0.009732152863095204\n",
            "Epoch 71 completed out of 100 loss: 0.009700567467759052\n",
            "Epoch 72 completed out of 100 loss: 0.009705491481969754\n",
            "Epoch 73 completed out of 100 loss: 0.00972979336977005\n",
            "Epoch 74 completed out of 100 loss: 0.009696950893849135\n",
            "Epoch 75 completed out of 100 loss: 0.009686476588249206\n",
            "Epoch 76 completed out of 100 loss: 0.009676256912449995\n",
            "Epoch 77 completed out of 100 loss: 0.00965637670829892\n",
            "Epoch 78 completed out of 100 loss: 0.009646275509148837\n",
            "Epoch 79 completed out of 100 loss: 0.009646791045864423\n",
            "Epoch 80 completed out of 100 loss: 0.009589178388317425\n",
            "Epoch 81 completed out of 100 loss: 0.009546666052192449\n",
            "Epoch 82 completed out of 100 loss: 0.009490527572731177\n",
            "Epoch 83 completed out of 100 loss: 0.009467072015007337\n",
            "Epoch 84 completed out of 100 loss: 0.009469162368526062\n",
            "Epoch 85 completed out of 100 loss: 0.009496144174287716\n",
            "Epoch 86 completed out of 100 loss: 0.009475046383837858\n",
            "Epoch 87 completed out of 100 loss: 0.009466724780698618\n",
            "Epoch 88 completed out of 100 loss: 0.009486780383934577\n",
            "Epoch 89 completed out of 100 loss: 0.009552126297106346\n",
            "Epoch 90 completed out of 100 loss: 0.009583168892810742\n",
            "Epoch 91 completed out of 100 loss: 0.00953171789025267\n",
            "Epoch 92 completed out of 100 loss: 0.009496622247000536\n",
            "Epoch 93 completed out of 100 loss: 0.00950794605538249\n",
            "Epoch 94 completed out of 100 loss: 0.009490937646478414\n",
            "Epoch 95 completed out of 100 loss: 0.00943245845536391\n",
            "Epoch 96 completed out of 100 loss: 0.009404980018734932\n",
            "Epoch 97 completed out of 100 loss: 0.009450006354600192\n",
            "Epoch 98 completed out of 100 loss: 0.00943775507931908\n",
            "Epoch 99 completed out of 100 loss: 0.00936613895619909\n",
            "Epoch 100 completed out of 100 loss: 0.009314735401421786\n",
            "\n",
            "Mean SNR: 11.440279529492061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWsZERIVlGPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  for i in range(len(mag_tex_list)):\n",
        "\n",
        "    mag_tex_value = np.array([mag_tex_list[i].T])\n",
        "    tex_value = np.array([tex_list[i].T])\n",
        "    \n",
        "    test_output = sess.run(prediction,feed_dict = {x: mag_tex_value})\n",
        "    test_predict = np.multiply(test_output,tex_value)\n",
        "    test_predict = test_predict.T[:,:,0]\n",
        "\n",
        "    test_hat = librosa.istft(test_predict , hop_length=512 , win_length=1024)\n",
        "    \n",
        "    file_num = str(i).zfill(4)\n",
        "    librosa.output.write_wav('test_audio_file_' + file_num + '.wav', test_hat, sr = 16000)\n",
        "    files.download('test_audio_file_' + file_num + '.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}